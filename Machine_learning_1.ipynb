{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTa3NuqFja0H"
      },
      "source": [
        "1 What is a parameter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvC7bH37jfVJ"
      },
      "source": [
        "In feature engineering, a parameter refers to a configurable value or setting that influences how raw data is transformed into meaningful features used for machine learning models. These parameters control the behavior of various feature transformation techniques, such as scaling, encoding, binning, and aggregating. For example, when creating a rolling average feature from time series data, the size of the rolling window is a parameter that determines how many past values are averaged. Similarly, when discretizing a continuous variable into bins, the number of bins or the thresholds used are parameters. These values are typically set by the data scientist based on domain knowledge or experimentation and are not learned from the data itself. Unlike model parameters, which are adjusted during model training, feature engineering parameters shape the structure and quality of input data before it reaches the model, playing a crucial role in the model’s performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_XkUs1YjyoK"
      },
      "source": [
        "2 What is correlation?\n",
        "What does negative correlation mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqft-cJej2P1"
      },
      "source": [
        " Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how changes in one variable are associated with changes in another. A correlation value ranges from -1 to 1, where 1 means a perfect positive correlation, -1 means a perfect negative correlation, and 0 indicates no correlation at all. A negative correlation means that as one variable increases, the other tends to decrease. For example, if the number of hours spent watching TV increases and test scores tend to decrease, there is a negative correlation between TV time and academic performance. Negative correlation does not imply causation, but it shows an inverse relationship between the variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtFEyPOEj7a6"
      },
      "source": [
        "3 Define Machine Learning. What are the main components in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7yakSjjj-e3"
      },
      "source": [
        " Machine Learning (ML) is a field of artificial intelligence that focuses on developing systems that can learn from data, identify patterns, and make decisions or predictions with minimal human intervention. Instead of being explicitly programmed to perform specific tasks, ML models improve their performance over time by learning from examples. The main components of machine learning include data, which serves as the input for training and evaluation; features, which are the relevant variables or attributes extracted from the data to help the model learn effectively; models, which are algorithms that learn patterns from the data to make predictions or decisions; and learning algorithms, which guide how the model learns from the data by minimizing errors. Additionally, evaluation metrics are used to assess how well the model performs on unseen data, ensuring its reliability and accuracy. Together, these components form the foundation of any machine learning system.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G65wpTkLkHc0"
      },
      "source": [
        "4 How does loss value help in determining whether the model is good or not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRn-zpxukKZO"
      },
      "source": [
        "The loss value is a key indicator of how well a machine learning model is performing, as it measures the difference between the model's predicted outputs and the actual target values. A lower loss value indicates that the model's predictions are close to the true values, suggesting better performance, while a higher loss value means the model is making larger errors. During training, the model adjusts its internal parameters to minimize this loss, which ideally leads to improved accuracy and generalization. Monitoring the loss value helps determine whether the model is learning effectively, overfitting (very low training loss but high validation loss), or underfitting (consistently high loss on both training and validation data). Therefore, the loss function serves as a crucial tool in guiding model optimization and evaluating its quality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYKn06mTkndo"
      },
      "source": [
        "5 What are continuous and categorical variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsmT0W_3kqqB"
      },
      "source": [
        " Continuous and categorical variables are two fundamental types of data used in statistics and machine learning. Continuous variables are numerical and can take any value within a given range, often including decimals or fractions. They represent measurable quantities such as height, weight, temperature, or income. These variables can have an infinite number of possible values. In contrast, categorical variables represent distinct groups or categories and are usually non-numeric. They describe qualities or characteristics such as gender, color, product type, or education level. Categorical variables can be further divided into nominal (no inherent order, like colors) and ordinal (with a meaningful order, like education levels). Understanding the difference between these two types is important for choosing the right data preprocessing and modeling techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV1hXxtnkyek"
      },
      "source": [
        "6 How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5du3zhxk4_i"
      },
      "source": [
        " Handling categorical variables in machine learning is essential because most algorithms require numerical input. To convert categorical data into a usable format, several encoding techniques are commonly used. One of the most popular methods is one-hot encoding, which creates binary columns for each category, assigning a value of 1 to the present category and 0 to others. Another approach is label encoding, where each category is assigned a unique integer; however, this can unintentionally introduce an ordinal relationship where none exists. For ordinal categorical variables, where the categories have a meaningful order (like \"low,\" \"medium,\" \"high\"), ordinal encoding preserves that order using numerical values. In high-cardinality situations (many unique categories), techniques like target encoding or frequency encoding may be used to reduce dimensionality and improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc_yc564lDIv"
      },
      "source": [
        "7 What do you mean by training and testing a dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-9mzhC8lGKn"
      },
      "source": [
        " Training and testing a dataset are fundamental steps in building and evaluating machine learning models. When developing a model, the original dataset is typically split into two parts: the training set and the testing set. The training set is used to teach the model by allowing it to learn patterns, relationships, and features from the data. The model adjusts its internal parameters during this phase to minimize errors and improve accuracy. Once the model has been trained, it is then evaluated on the testing set, which contains data the model has never seen before. This step helps assess how well the model generalizes to new, unseen data and provides an unbiased estimate of its real-world performance. Separating data into training and testing sets is crucial to prevent overfitting, where a model performs well on training data but poorly on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIspW11ulLvd"
      },
      "source": [
        "8 What is sklearn.preprocessing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SECp2qqhlOXn"
      },
      "source": [
        " sklearn.preprocessing is a module in the popular Python library scikit-learn that provides a variety of tools and functions for preparing and transforming data before feeding it into machine learning models. This module includes techniques for scaling features (such as normalization and standardization), encoding categorical variables, handling missing values, and generating polynomial features, among others. Proper preprocessing is essential because many machine learning algorithms perform better or converge faster when the input data is scaled or transformed appropriately. By using sklearn.preprocessing, data scientists can efficiently clean and transform raw data into a format that improves model training and prediction accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeTkPIH9lU4F"
      },
      "source": [
        "9 What is a Test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf_LRzd6lXNN"
      },
      "source": [
        "- A test set is a portion of a dataset reserved specifically for evaluating the performance of a machine learning model after it has been trained. Unlike the training set, which the model uses to learn patterns and relationships in the data, the test set contains new, unseen examples that help assess how well the model generalizes to real-world data. By measuring metrics like accuracy, precision, or error on the test set, we can estimate the model’s effectiveness and reliability when making predictions on data it hasn’t encountered before. Using a separate test set is crucial to avoid overly optimistic evaluations and to ensure that the model is not just memorizing the training data but truly learning meaningful patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvJDH45ElbRr"
      },
      "source": [
        "10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4xsLyBXle_o"
      },
      "source": [
        "- In Python, data splitting for model fitting is commonly done using the train_test_split function from the sklearn.model_selection module. This function randomly divides the dataset into training and testing subsets based on a specified ratio, such as 70% for training and 30% for testing. The training set is used to teach the machine learning model by allowing it to learn patterns from the data, while the testing set is reserved for evaluating how well the model performs on unseen data. This split ensures that the model’s ability to generalize can be accurately assessed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syOxndMSljw5"
      },
      "source": [
        "11 Why do we have to perform EDA before fitting a model to the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9qrgPT9lnDu"
      },
      "source": [
        "- Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential because it helps you understand the underlying patterns, relationships, and quality of the data. Through EDA, you can identify important features, detect anomalies, outliers, or missing values, and uncover trends that might influence how the model should be built. This process also helps in selecting the right preprocessing steps, such as scaling or encoding, and guides feature engineering to improve model performance. Without EDA, you risk feeding noisy, biased, or irrelevant data into the model, which can lead to poor predictions and unreliable results. In short, EDA provides valuable insights that inform better decisions throughout the modeling process, ultimately leading to more accurate and robust machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO_OVnjmlreZ"
      },
      "source": [
        "12 What is correlation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-COBHkOltco"
      },
      "source": [
        "- Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It indicates how changes in one variable are associated with changes in another, helping to understand whether they move together or in opposite directions. Correlation values range from -1 to 1, where a value close to 1 means a strong positive relationship (both variables increase or decrease together), a value close to -1 indicates a strong negative relationship (one variable increases while the other decreases), and a value around 0 suggests no meaningful relationship. Understanding correlation is important in data analysis and machine learning because it helps identify which variables might influence each other and guides feature selection or engineering.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIN1a8Y8lwlW"
      },
      "source": [
        "13 What does negative correlation mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKugLs_UlzUN"
      },
      "source": [
        "- Negative correlation means that two variables move in opposite directions: when one variable increases, the other tends to decrease, and vice versa. This inverse relationship is quantified by a correlation value less than zero, typically between -1 and 0. A correlation close to -1 indicates a strong negative correlation, meaning the variables are closely linked but move oppositely. For example, if the number of hours spent watching TV increases while a student’s test scores decrease, these two variables have a negative correlation. Understanding negative correlations is important because it helps reveal how variables may counterbalance each other, providing valuable insights in data analysis and informing better decision-making in modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SzzCwazl2Yw"
      },
      "source": [
        "14 How can you find correlation between variables in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILJHciJAl7Iw"
      },
      "source": [
        "- In Python, you can find the correlation between variables using libraries like Pandas and NumPy, which offer built-in functions to calculate correlation coefficients. The most common method is the Pearson correlation, which measures the linear relationship between two continuous variables. Using Pandas, you can easily compute this by calling the .corr() method on a DataFrame, which returns a correlation matrix showing pairwise correlations between all variables. For example, df.corr() provides the Pearson correlation coefficients between each pair of numerical columns in the dataset. You can also calculate the correlation between two specific variables using df['variable1'].corr(df['variable2']). Other correlation methods, such as Spearman or Kendall, are also available if you need to measure non-linear or rank-based relationships. These tools make it straightforward to analyze how variables relate to one another in your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3n74FPJmBUq"
      },
      "source": [
        "15 What is causation? Explain difference between correlation and causation with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrAOPmNmILf"
      },
      "source": [
        "- Causation refers to a relationship where one event or variable directly causes a change in another. It implies a cause-and-effect connection, meaning that changes in the cause lead to changes in the effect. In contrast, correlation simply indicates that two variables move together in some way, either positively or negatively, but it does not prove that one causes the other. For example, there might be a correlation between ice cream sales and drowning incidents because both increase during summer, but buying ice cream does not cause drowning; instead, the warmer weather causes both to rise independently. This highlights the important difference: correlation shows a relationship, while causation explains why that relationship exists. Understanding this distinction is critical in data analysis to avoid making incorrect assumptions or conclusions based solely on correlated data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77HdtuVBmL4n"
      },
      "source": [
        "16 What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGr-0CdAmOgO"
      },
      "source": [
        "- An optimizer is an algorithm used in machine learning and deep learning to adjust the parameters of a model, such as weights, in order to minimize the loss function and improve the model’s performance. It guides the learning process by iteratively updating the parameters based on the calculated gradients, helping the model converge to the best solution. There are several types of optimizers, each with its own approach. Gradient Descent is the simplest, where parameters are updated in the direction of the negative gradient of the loss function using the entire training dataset. Stochastic Gradient Descent (SGD) improves on this by updating parameters using one training example at a time, which can speed up training but introduces more noise. Mini-batch Gradient Descent is a compromise, using small batches of data for each update to balance speed and stability. More advanced optimizers like Adam (Adaptive Moment Estimation) combine ideas from momentum and adaptive learning rates, adjusting the step size for each parameter dynamically, which often leads to faster and more reliable convergence. For example, Adam is widely used in deep learning because it efficiently handles sparse gradients and noisy data. Choosing the right optimizer depends on the problem, data, and model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVLVaQ1PmSXe"
      },
      "source": [
        "17 What is sklearn.linear_model ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqOG7GQumV58"
      },
      "source": [
        "- sklearn.linear_model is a module in the scikit-learn Python library that provides a variety of linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable, making them simple yet powerful tools for many problems. The module includes algorithms like Linear Regression for predicting continuous outcomes, Logistic Regression for binary classification, and regularized models like Ridge and Lasso that help prevent overfitting by adding penalties to the model coefficients. Using sklearn.linear_model, data scientists can quickly build, train, and evaluate linear models with built-in methods for fitting data and making predictions, making it a foundational part of many machine learning workflows.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfoC6bDmmalQ"
      },
      "source": [
        "18 What does model.fit() do? What arguments must be given?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPbq7NUlmdbM"
      },
      "source": [
        "- The method model.fit() in machine learning libraries like scikit-learn is used to train a model by learning the relationship between the input data and the target variable. When you call fit(), the model processes the training data, adjusts its internal parameters, and optimizes itself to best capture the patterns needed for accurate predictions. Typically, fit() requires at least two arguments: the feature data (usually represented as a 2D array or DataFrame) and the target labels or values (a 1D array or Series). For example, in supervised learning, you pass X (input features) and y (output labels) to fit(), allowing the model to learn how to map inputs to outputs. This step is essential before using the model to make predictions or evaluate its performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fPizsbMmgwE"
      },
      "source": [
        "19 What does model.predict() do? What arguments must be given?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xcdR4R1mjxg"
      },
      "source": [
        "- The method model.predict() is used to generate predictions from a trained machine learning model. After the model has been fitted to the training data, calling predict() allows you to input new, unseen feature data and obtain the model’s predicted outputs based on what it has learned. The primary argument required is the feature data (X), typically provided as a 2D array or DataFrame containing the same set of features used during training. The method then processes this input through the model’s learned parameters and returns predictions, which could be continuous values in regression tasks or class labels in classification problems. Using predict() is a crucial step in applying a model to real-world data for decision-making or evaluation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EBwWmzmm0d"
      },
      "source": [
        "20 What are continuous and categorical variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J5j0zp3mpxo"
      },
      "source": [
        "- Continuous and categorical variables are two primary types of data used in statistics and machine learning. Continuous variables represent measurable quantities that can take any value within a range, including decimals and fractions, such as height, temperature, or weight. They capture information on a scale and can be meaningfully ordered and measured. On the other hand, categorical variables represent distinct groups or categories and typically describe qualities or characteristics that are not inherently numerical, such as gender, color, or type of product. Categorical variables can be nominal, where categories have no natural order (like eye color), or ordinal, where categories follow a meaningful sequence (like education level). Understanding these variable types is essential because they determine how data should be processed and analyzed in machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5UtzZfmtT2"
      },
      "source": [
        "Ques21) What is feature scaling? How does it help in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jwZi2xymwRV"
      },
      "source": [
        "- Feature scaling is a data preprocessing technique that involves transforming the values of numerical features to a common scale, often within a specific range like 0 to 1 or with a standardized distribution (mean of zero and standard deviation of one). This process is important in machine learning because many algorithms—such as gradient descent-based models, support vector machines, and k-nearest neighbors—are sensitive to the scale of input features. Without scaling, features with larger numeric ranges can disproportionately influence the model, leading to biased results or slower convergence during training. By applying feature scaling, all features contribute more equally to the model’s learning process, improving performance, stability, and the speed of convergence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OsEGmwhmzUS"
      },
      "source": [
        "22 How do we perform scaling in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJHhvxhYm2A0"
      },
      "source": [
        "- In Python, feature scaling is typically performed using the sklearn.preprocessing module from the scikit-learn library, which provides convenient tools for standardizing or normalizing data. Two common methods are StandardScaler, which transforms the data to have a mean of zero and a standard deviation of one, and MinMaxScaler, which scales the data to a specified range, usually between 0 and 1. To perform scaling, you first create an instance of the scaler, then use the fit_transform() method to both learn the scaling parameters from the training data and apply the transformation. This process ensures that all features contribute equally to the machine learning model and helps improve its performance and convergence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iosFQ7N5m7gN"
      },
      "source": [
        "23 What is sklearn.preprocessing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zyw4yrtm93M"
      },
      "source": [
        "- sklearn.preprocessing is a module in the scikit-learn Python library that provides a variety of tools and functions for preparing and transforming data before feeding it into machine learning models. This module includes techniques for scaling numerical features (such as standardization and normalization), encoding categorical variables, handling missing values, and generating polynomial or interaction features. Proper preprocessing is crucial because many machine learning algorithms perform better and converge faster when the input data is scaled or transformed appropriately. By using sklearn.preprocessing, data scientists can efficiently clean and transform raw data into a format that enhances model training and improves overall predictive performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKgbSC6EnA5Y"
      },
      "source": [
        "24 How do we split data for model fitting (training and testing) in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtcfNaMbnDWU"
      },
      "source": [
        "- In Python, data is typically split into training and testing sets using the train_test_split function from the sklearn.model_selection module. This function allows you to divide your dataset into two parts: one for training the machine learning model and the other for evaluating its performance on unseen data. You specify the proportion of data to allocate to each set, such as 70% for training and 30% for testing, ensuring that the split is random and representative of the overall dataset. This separation is important to prevent overfitting and to provide an unbiased assessment of how well the model generalizes to new data. Using train_test_split is a straightforward and effective way to prepare data for model fitting in Python.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZc6T_tnGlg"
      },
      "source": [
        "25 Explain data encoding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BAPA6hSnJ1_"
      },
      "source": [
        "- Data encoding is the process of converting categorical variables into a numerical format that machine learning algorithms can understand and work with. Since most algorithms require numerical input, categorical data—such as colors, types, or labels—must be transformed into numbers without losing their meaningful information. Common encoding techniques include label encoding, which assigns a unique integer to each category, and one-hot encoding, which creates binary columns representing the presence or absence of each category. Proper encoding is crucial because it enables models to interpret categorical features correctly and can significantly impact the accuracy and performance of the machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
